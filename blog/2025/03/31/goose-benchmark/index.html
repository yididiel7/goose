<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Community-Inspired Benchmarking: The Goose Vibe Check | codename goose</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" name="description" content="See how open source AI models measure up in our first Goose agent benchmark tests"><meta data-rh="true" property="article:published_time" content="2025-03-31T00:00:00.000Z"><meta data-rh="true" property="og:title" content="Community-Inspired Benchmarking: The Goose Vibe Check"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="og:url" content="https://block.github.io/goose/blog/2025/03/31/goose-benchmark"><meta data-rh="true" property="og:description" content="See how open source AI models measure up in our first Goose agent benchmark tests"><meta data-rh="true" property="og:image" content="http://block.github.io/goose/assets/images/goose-benchmark-d9726c203290ef892fe3fe3adc7d898f.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="twitter:domain" content="block.github.io/goose"><meta data-rh="true" name="twitter:title" content="Community-Inspired Benchmarking: The Goose Vibe Check"><meta data-rh="true" name="twitter:description" content="See how open source AI models measure up in our first Goose agent benchmark tests"><meta data-rh="true" name="twitter:image" content="http://block.github.io/goose/assets/images/goose-benchmark-d9726c203290ef892fe3fe3adc7d898f.png"><link data-rh="true" rel="icon" href="/goose/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://block.github.io/goose/blog/2025/03/31/goose-benchmark"><link data-rh="true" rel="alternate" href="https://block.github.io/goose/blog/2025/03/31/goose-benchmark" hreflang="en"><link data-rh="true" rel="alternate" href="https://block.github.io/goose/blog/2025/03/31/goose-benchmark" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://block.github.io/goose/blog/2025/03/31/goose-benchmark","mainEntityOfPage":"https://block.github.io/goose/blog/2025/03/31/goose-benchmark","url":"https://block.github.io/goose/blog/2025/03/31/goose-benchmark","headline":"Community-Inspired Benchmarking: The Goose Vibe Check","name":"Community-Inspired Benchmarking: The Goose Vibe Check","description":"See how open source AI models measure up in our first Goose agent benchmark tests","datePublished":"2025-03-31T00:00:00.000Z","author":{"@type":"Person","name":"Alice Hau","description":"Machine Learning Engineer","image":"https://avatars.githubusercontent.com/u/110418948?v=4"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://block.github.io/goose/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/goose/blog/rss.xml" title="codename goose RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/goose/blog/atom.xml" title="codename goose Atom Feed"><link rel="stylesheet" href="/goose/assets/css/styles.51ea72d1.css">
<script src="/goose/assets/js/runtime~main.483e2003.js" defer="defer"></script>
<script src="/goose/assets/js/main.3d3778ed.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/goose/img/logo_light.png"><link rel="preload" as="image" href="/goose/img/logo_dark.png"><link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/110418948?v=4"><link rel="preload" as="image" href="/goose/assets/images/claude-3-5-haiku-ff0b0347d5aec59efab98769039f7063.gif"><link rel="preload" as="image" href="/goose/assets/images/claude-3-5-sonnet-2-b445a205684b5c7bd674d744b92cc4ea.gif"><link rel="preload" as="image" href="/goose/assets/images/claude-3-7-sonnet-a11804cddf505cec1df78ca8315c8257.gif"><link rel="preload" as="image" href="/goose/assets/images/deepseek-chat-v3-0324-01d4dec7f79311b593ac04fbba9f93ef.gif"><link rel="preload" as="image" href="/goose/assets/images/deepseek-r1-toolshim-mistral-nemo-bf4bd9d06ff57563ae2762d047ccb1c7.gif"><link rel="preload" as="image" href="/goose/assets/images/gpt-4-5-preview-1cd4983ce3ceafaf22b12dff95ed4ad1.gif"><link rel="preload" as="image" href="/goose/assets/images/gpt-4o-mini-9e4478259ad6e3e4353394c0b3f19126.gif"><link rel="preload" as="image" href="/goose/assets/images/gpt-4o-cbdbf0c788605e797fb8f4ea9b987b8c.gif"><link rel="preload" as="image" href="/goose/assets/images/o1-134e3913d813a9e797e979c42a92fa3f.gif"><link rel="preload" as="image" href="/goose/assets/images/o3-mini-7a869a80bf99f27dd7454830ae309184.gif"><link rel="preload" as="image" href="/goose/assets/images/qwen2.5-coder-32b-ceca4ac06aaf0f42f3d6b65363c60e3e.gif"><link rel="preload" as="image" href="/goose/assets/images/qwq-3a05f474ec77049ba53a0439b36df0ba.gif"><link rel="preload" as="image" href="/goose/assets/images/gemma3.27b-toolshim-mistral-nemo-ee63d6bc421659fa3501a447f8c46426.png"><link rel="preload" as="image" href="/goose/assets/images/claude-3.5-haiku-71f1ce5faa13846a556ba539c57caae8.png"><link rel="preload" as="image" href="/goose/assets/images/claude-3.5-sonnet-2-2c683339b1e4880de5ce5aa435304a5a.png"><link rel="preload" as="image" href="/goose/assets/images/claude-3.7-sonnet-6042d99d5829957f3fd471d44ff2ea26.png"><link rel="preload" as="image" href="/goose/assets/images/deepseek-chat-v3-0324-879b3ea78b30ff135607365fd77eeebe.png"><link rel="preload" as="image" href="/goose/assets/images/deepseek-r1-distill-llama-70b-toolshim-mistral-nemo-bfd7f9d3f37a92aaae6c7afb74b52d91.png"><link rel="preload" as="image" href="/goose/assets/images/gpt-4.5-preview-99cafaf409ffbc47a8186ed0a5c05043.png"><link rel="preload" as="image" href="/goose/assets/images/gpt-4o-mini-37fbaf84ea5c2d7d57a9b4e7ddf4fb3b.png"><link rel="preload" as="image" href="/goose/assets/images/gpt-4o-8b238b9d92ca62e7480ab6f12faeedfa.png"><link rel="preload" as="image" href="/goose/assets/images/llama3.3.70b-instruct-q4_K_M-91e7e0436dd5b7530fc18ff39366369d.png"><link rel="preload" as="image" href="/goose/assets/images/llama3.3.70b-instruct-q8_0-9477c2fcd3abc509fa4855d9633be812.png"><link rel="preload" as="image" href="/goose/assets/images/mistral-nemo_index-4866f9a20a1294585adef2dc4fbc4ade.png"><link rel="preload" as="image" href="/goose/assets/images/o1-9a41312ec15fa512b13f215c94dc1335.png"><link rel="preload" as="image" href="/goose/assets/images/o3-mini-0da4fd28bcb174906ff6b2664c08e88d.png"><link rel="preload" as="image" href="/goose/assets/images/phi4-toolshim-mistral-nemo-db0e5f6075b793b2b6facfa483fc9a5d.png"><link rel="preload" as="image" href="/goose/assets/images/phi4-toolshim-qwen2.5-coder7b-632ebfc62d7e69db76bc030235990574.png"><link rel="preload" as="image" href="/goose/assets/images/qwen2.5-coder.14b-dfb90e9a247737df599d7736e982fa64.png"><link rel="preload" as="image" href="/goose/assets/images/qwen2.5-coder.32b-e540b5187a35326045a1810070188b68.png"><link rel="preload" as="image" href="/goose/assets/images/qwen2.5.14b-5f613200d90cb8f8e5242cefab130ed2.png"><link rel="preload" as="image" href="/goose/assets/images/qwen2.5.32b-cf589a3b4494e4f75a63f3b7c8f13923.png"><link rel="preload" as="image" href="/goose/assets/images/qwq-2df978b1c23d170ed2bf2d2caf5c3ecd.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/goose/"><div class="navbar__logo"><img src="/goose/img/logo_light.png" alt="Block Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/goose/img/logo_dark.png" alt="Block Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/goose/docs/quickstart">Quickstart</a><a class="navbar__item navbar__link" href="/goose/extensions">Extensions</a><a class="navbar__item navbar__link" href="/goose/docs/category/getting-started">Docs</a><a class="navbar__item navbar__link" href="/goose/docs/category/tutorials">Tutorials</a><a class="navbar__item navbar__link" href="/goose/prompt-library">Prompt Library</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/goose/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://discord.gg/block-opensource" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/block/goose" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div id="inkeep"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/23/things-need-to-know">4 Things You Need to Know Before Using Goose</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/22/community-bestcodes">How One Contribution Can Spark Many Wins</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/22/mcp-is-rewriting-the-rules-of-api-integration">MCP Is Rewriting the Rules of API Integration</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/21/mcp-in-enterprise">MCP in the Enterprise: Real World Adoption at Block</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/21/practical-use-cases-of-ai">11 Practical Ways I Use AI Agents Without Losing My Authenticity</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/17/goose-goes-to-NY">Codename Goose Goes to New York</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/14/community-atruelight4">How ATrueLight4 Helped Goose Navigate Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/11/finetuning-toolshim">Finetuning Toolshim Models for Tool Calling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/10/visual-guide-mcp">A Visual Guide To MCP Ecosystem</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/08/vibe-code-responsibly">How to Vibe Code Responsibly (with Goose)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/01/top-5-mcp-servers">Top 5 MCP Servers I Use as a Developer with Goose</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/04/01/mcp-nondevs">MCP Explained for Non-Developers</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/goose/blog/2025/03/31/goose-benchmark">Community-Inspired Benchmarking: The Goose Vibe Check</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/31/securing-mcp">Securing the Model Context Protocol</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/28/vibe-coding-with-goose">Vibe Coding with Goose and the Speech MCP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/26/mcp-security">How to Determine If An MCP Server Is Safe</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/21/goose-boston-meetup">Codename Goose Goes to Boston</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/21/goose-vscode">Cracking the Code with VS Code MCP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/20/asana-calendar-mcp">How I Use Goose to Plan My Week with Asana and Google Calendar MCPs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/19/better-ai-prompting">AI Prompting 101: How to Get the Best Responses from Your AI Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/18/goose-langfuse">How Goose Catches AI Errors with Langfuse</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/14/goose-ollama">AI, But Make It Local With Goose and Ollama</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/12/goose-figma-mcp">Turn Figma Designs Into Code With Goose</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/10/goose-calls-vyop">Automating Phone Calls with Goose</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/03/06/goose-tips">6 Essential Tips for Working with Goose</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/02/21/gooseteam-mcp">Let A Team of AI Agents Do It For You</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/02/17/agentic-ai-mcp">Agentic AI and the MCP Ecosystem</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2025/01/28/introducing-codename-goose">Introducing codename goose</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2024/12/11/resolving-ci-issues-with-goose-a-practical-walkthrough">Resolving CI Issues with Goose: A Practical Walkthrough</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2024/12/10/connecting-ai-agents-to-your-systems-with-mcp">Connecting AI Agents to Your Systems with MCP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2024/12/06/previewing-goose-v10-beta">Previewing Goose v1.0 Beta</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/goose/blog/2024/11/22/screenshot-driven-development">Screenshot-Driven Development</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Community-Inspired Benchmarking: The Goose Vibe Check</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-03-31T00:00:00.000Z">March 31, 2025</time> · <!-- -->20 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/goose/blog/authors/alice"><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/110418948?v=4" alt="Alice Hau"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/goose/blog/authors/alice"><span class="authorName_yefp">Alice Hau</span></a></div><small class="authorTitle_nd0D" title="Machine Learning Engineer">Machine Learning Engineer</small><div class="authorSocials_rSDt"><a href="https://www.linkedin.com/in/alice-hau/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" class="authorSocialLink_owbf"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453" fill="#0A66C2"></path></svg></a><a href="https://github.com/alicehau" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialLink_owbf githubSvg_Uu4N"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p><img decoding="async" loading="lazy" alt="blog cover" src="/goose/assets/images/goose-benchmark-d9726c203290ef892fe3fe3adc7d898f.png" width="1200" height="630" class="img_ev3q"></p>
<p>We&#x27;ve been measuring Goose&#x27;s performance with various AI models, including a variety of popular open-source models that can run locally on consumer hardware (RTX 4080, Mac M-series). We understand that many in our community value a fully open-source, local experience without relying on cloud services.</p>
<p>This blog shares our findings comparing open-source models against their closed counterparts, highlighting both current performance gaps and paths for future improvement. Our benchmark is still in its early stages, but we wanted to release it as a starting point for distinguishing models that exhibit stronger agentic capabilities by their ability to pilot Goose (distinct from reasoning or other capabilities often captured in other popular benchmarks).</p>
<p>Our evaluations are inspired by grassroots efforts we&#x27;ve seen in communities like <a href="https://www.reddit.com/r/LocalLLaMA/" target="_blank" rel="noopener noreferrer">r/LocalLlama</a>. If you&#x27;ve spent time there, you’ve probably seen enthusiasts crowdsource model performance on standard tasks like &quot;build a flappy bird game&quot; or <a href="https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/" target="_blank" rel="noopener noreferrer">create a rotating hexagon with a bouncing ball</a>&quot; to quickly compare model performance.</p>
<p>These community evals aren&#x27;t the rigorous, peer-reviewed benchmarks that research labs publish in academic papers. However, they help provide quick, intuitive assessments of capabilities across different models and versions.</p>
<p>In this spirit, we&#x27;re introducing our <strong>Goose Vibe Check</strong> leaderboard.</p>
<p>Thank you to the Ollama team for their help and support in our experimentation contributing to this blog! We used Ollama’s <a href="https://ollama.com/blog/structured-outputs" target="_blank" rel="noopener noreferrer">structured outputs</a> feature to enable our <a href="https://block.github.io/goose/docs/guides/experimental-features/#ollama-tool-shim" target="_blank" rel="noopener noreferrer">toolshim implementation</a> (more below) and their recently released <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size" target="_blank" rel="noopener noreferrer">context length parameter override</a> to enable testing on longer contexts.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="leaderboard">Leaderboard<a href="#leaderboard" class="hash-link" aria-label="Direct link to Leaderboard" title="Direct link to Leaderboard">​</a></h2>
<table><thead><tr><th>Rank</th><th>Model</th><th>Average Eval Score</th><th>Inference Provider</th></tr></thead><tbody><tr><td>1</td><td>claude-3-5-sonnet-2</td><td>1.00</td><td>databricks (bedrock)</td></tr><tr><td>2</td><td>claude-3-7-sonnet</td><td>0.94</td><td>databricks (bedrock)</td></tr><tr><td>3</td><td>claude-3-5-haiku</td><td>0.91</td><td>databricks (bedrock)</td></tr><tr><td>4</td><td>o1</td><td>0.81</td><td>databricks (bedrock)</td></tr><tr><td>4</td><td>gpt-4o</td><td>0.81</td><td>databricks (bedrock)</td></tr><tr><td>6</td><td>qwen2.5-coder:32b</td><td>0.8</td><td>ollama</td></tr><tr><td>7</td><td>o3-mini</td><td>0.79</td><td>databricks (bedrock)</td></tr><tr><td>8</td><td>qwq</td><td>0.77</td><td>ollama</td></tr><tr><td>9</td><td>gpt-4o-mini</td><td>0.74</td><td>databricks (bedrock)</td></tr><tr><td>10</td><td>deepseek-chat-v3-0324</td><td>0.73</td><td>openrouter</td></tr><tr><td>11</td><td>gpt-4-5-preview</td><td>0.67</td><td>databricks</td></tr><tr><td>12</td><td>qwen2.5:32b</td><td>0.64</td><td>ollama</td></tr><tr><td>13</td><td>qwen2.5:14b</td><td>0.62</td><td>ollama</td></tr><tr><td>14</td><td>qwen2.5-coder:14b</td><td>0.51</td><td>ollama</td></tr><tr><td>15</td><td>deepseek-r1-toolshim-mistral-nemo*</td><td>0.48</td><td>openrouter</td></tr><tr><td>16</td><td>llama3.3:70b-instruct-q4_K_M</td><td>0.47</td><td>ollama</td></tr><tr><td>17</td><td>phi4-toolshim-mistral-nemo*</td><td>0.46</td><td>ollama</td></tr><tr><td>18</td><td>phi4-mistral-nemo</td><td>0.45</td><td>ollama</td></tr><tr><td>19</td><td>gemma3:27b-toolshim-mistral-nemo*</td><td>0.43</td><td>ollama</td></tr><tr><td>20</td><td>deepseek-r1-toolshim-qwen2.5-coder7b*</td><td>0.42</td><td>openrouter</td></tr><tr><td>21</td><td>llama3.3:70b-instruct-q8_0</td><td>0.41</td><td>ollama</td></tr><tr><td>22</td><td>deepseek-r1:14b-toolshim-mistral-nemo*</td><td>0.37</td><td>openrouter</td></tr><tr><td>23</td><td>deepseek-r1-distill-llama-70b-toolshim-mistral-nemo*</td><td>0.36</td><td>ollama</td></tr><tr><td>24</td><td>phi4-toolshim-qwen2.5-coder7b*</td><td>0.3</td><td>ollama</td></tr><tr><td>25</td><td>mistral-nemo</td><td>0.27</td><td>ollama</td></tr><tr><td>26</td><td>deepseek-r1-distill-llama-70b-toolshim-qwen2.5-coder7b*</td><td>0.26</td><td>openrouter</td></tr><tr><td>27</td><td>llama3.2</td><td>0.25</td><td>ollama</td></tr><tr><td>28</td><td>gemma3:27b-toolshim-qwen2.5-coder7b*</td><td>0.24</td><td>ollama</td></tr><tr><td>29</td><td>deepseek-r1:14b-toolshim-qwen2.5-coder7b*</td><td>0.22</td><td>ollama</td></tr><tr><td>29</td><td>gemma3:12b-toolshim-qwen2.5-coder7b*</td><td>0.22</td><td>ollama</td></tr><tr><td>31</td><td>mistral</td><td>0.17</td><td>ollama</td></tr><tr><td>32</td><td>gemma3:12b-toolshim-mistral-nemo*</td><td>0.15</td><td>ollama</td></tr></tbody></table>
<blockquote>
<p><em>Models with &#x27;toolshim&#x27; in their name indicate a Goose configuration using both a primary model and a secondary local Ollama model to interpret the primary model&#x27;s response into appropriate tools for Goose to invoke. Low performance may be indicative of the shim performance rather than the base model itself. We use toolshims for select models because all evaluations in this experiment require tool use capabilities, but not all models in our experiment natively support tool calling.</em></p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-source-model-details">Open Source Model Details<a href="#open-source-model-details" class="hash-link" aria-label="Direct link to Open Source Model Details" title="Direct link to Open Source Model Details">​</a></h2>
<table><thead><tr><th>Rank</th><th>Model</th><th>Model Params</th><th>Quantization</th></tr></thead><tbody><tr><td>1</td><td>qwen2.5-coder:32b</td><td>32B</td><td>Q4_K_M</td></tr><tr><td>2</td><td>qwq</td><td>32B</td><td>Q4_K_M</td></tr><tr><td>3</td><td>deepseek-chat-v3-0324</td><td>671B total, 37B active</td><td>-</td></tr><tr><td>4</td><td>qwen2.5:32b</td><td>32B</td><td>Q4_K_M</td></tr><tr><td>5</td><td>qwen2.5:14b</td><td>14B</td><td>Q4_K_M</td></tr><tr><td>6</td><td>qwen2.5-coder:14b</td><td>14B</td><td>Q4_K_M</td></tr><tr><td>7</td><td>deepseek-r1-toolshim-mistral-nemo</td><td>671B total, 37B active</td><td>fp8</td></tr><tr><td>8</td><td>llama3.3:70b-instruct-q4_K_M</td><td>70B</td><td>Q4_K_M</td></tr><tr><td>9</td><td>phi4-toolshim-mistral-nemo</td><td>14B</td><td>Q4_K_M</td></tr><tr><td>10</td><td>phi4-mistral-nemo</td><td>14B</td><td>Q4_K_M</td></tr><tr><td>11</td><td>gemma3:27b-toolshim-mistral-nemo</td><td>27B</td><td>Q4_K_M</td></tr><tr><td>12</td><td>deepseek-r1-toolshim-qwen2.5-coder7b</td><td>671B total, 37B active</td><td>fp8</td></tr><tr><td>13</td><td>llama3.3:70b-instruct-q8_0</td><td>70B</td><td>Q8_0</td></tr><tr><td>14</td><td>deepseek-r1:14b-toolshim-mistral-nemo</td><td>14B</td><td>Q4_K_M</td></tr><tr><td>15</td><td>deepseek-r1-distill-llama-70b-toolshim-mistral-nemo</td><td>70B</td><td>-</td></tr><tr><td>16</td><td>phi4-toolshim-qwen2.5-coder7b</td><td>14B</td><td>Q4_K_M</td></tr><tr><td>17</td><td>mistral-nemo</td><td>12B</td><td>Q4_0</td></tr><tr><td>18</td><td>deepseek-r1-distill-llama-70b-toolshim-qwen2.5-coder7b</td><td>70B</td><td>-</td></tr><tr><td>19</td><td>llama3.2</td><td>3B</td><td>Q4_K_M</td></tr><tr><td>20</td><td>gemma3:27b-toolshim-qwen2.5-coder7b</td><td>27B</td><td>Q4_K_M</td></tr><tr><td>21</td><td>deepseek-r1:14b-toolshim-qwen2.5-coder7b</td><td>14B</td><td>Q4_K_M</td></tr><tr><td>21</td><td>gemma3:12b-toolshim-qwen2.5-coder7b</td><td>12B</td><td>Q4_K_M</td></tr><tr><td>23</td><td>mistral</td><td>7B</td><td>Q8_0</td></tr><tr><td>24</td><td>gemma3:12b-toolshim-mistral-nemo</td><td>12B</td><td>Q4_K_M</td></tr></tbody></table>
<p><img decoding="async" loading="lazy" alt="Open Model Performance by Parameter Size" src="/goose/assets/images/model_sizes_vs_score-6512491865ea8ed96119245f1f21687e.png" width="5370" height="7171" class="img_ev3q"></p>
<blockquote>
<p><em>This chart presents a view of open model performance across different parameter sizes. In the 15-32B category, we see particularly impressive results from models like qwen2.5-coder:32b (0.80) and qwq (0.77). The chart also highlights the performance gap between models with native tool calling capabilities versus those requiring toolshim implementation (shown with dotted lines), a gap which appears consistent across all size categories. This suggests that native tool calling capabilities significantly impact performance on agentic tasks. With targeted improvements in tool calling capabilities, larger open models could potentially close the performance gap with closed-source alternatives in agentic settings.</em></p>
</blockquote>
<p><img decoding="async" loading="lazy" alt="Token Usage vs Scores" src="/goose/assets/images/tokens_vs_score-729f597c3ada0924c0c400b17459a3f3.png" width="3569" height="2970" class="img_ev3q"></p>
<blockquote>
<p><em>This scatterplot shows Claude models achieving top scores (0.9+) regardless of token usage, while open source models like qwen2.5-coder:32b perform well with moderate token consumption. Toolshimmed models consistently score lower, suggesting the toolshims are not very effective at closing the gap in native tool support between models. Higher token consumption up to a point appears to generally improve performance.</em></p>
</blockquote>
<p><img decoding="async" loading="lazy" alt="Tool Calls vs Scores" src="/goose/assets/images/tool_calls_vs_score-bb8723cbd0a4509b1776776fb54ec07b.png" width="3568" height="2970" class="img_ev3q"></p>
<blockquote>
<p><em>Models with either too few or excessive tool calls score lower, indicating effective tool utilization - not just frequency - correlates with improved performance. Toolshimmed models for the most part invoke fewer tool calls, suggesting that the toolshims are not sufficient in their current implementation to make models effective at correctly calling the right tools.</em></p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-results">Key Results<a href="#key-results" class="hash-link" aria-label="Direct link to Key Results" title="Direct link to Key Results">​</a></h2>
<ol>
<li>
<p><strong>Closed models currently lead</strong>: Closed source models like Claude and GPT models still generally lead open source alternatives in agentic tasks.</p>
</li>
<li>
<p><strong>Promising open challengers</strong>: Models like the Qwen series and DeepSeek-v3 show significant promise among open source alternatives, but they have not yet reached the consistency and reliability of closed models across all tasks.</p>
</li>
<li>
<p><strong>Token efficiency matters</strong>: Some open models can achieve good performance while using fewer tokens, which can translate to faster task completion times and potentially lower cost. Claude-3-7-sonnet exhibits strong performance alongside claude-3-5-sonnet-2, but at much greater token usage.</p>
</li>
<li>
<p><strong>Tool calling is crucial but not as reliable in open source models today</strong>: Effective tool calling remains a significant differentiator in agentic model performance. Open source models still struggle with generating structured tool calls reliably, limiting their effectiveness on complex tasks.</p>
</li>
<li>
<p><strong>More comprehensive and complex eval tasks are needed to further stratify the top performers:</strong> Our current evaluation suite, consisting of only eight tasks (ran 3x), may be too limited to effectively differentiate top-performing models. Several models clustered around similar scores in the .77-.81 range, likely due to the simplicity of the tasks, which require minimal complex reasoning. Expanding the evaluations to include more sophisticated tasks would provide further stratification and allow the models to better showcase their more or less advanced capabilities.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approach-and-methodology">Approach and Methodology<a href="#approach-and-methodology" class="hash-link" aria-label="Direct link to Approach and Methodology" title="Direct link to Approach and Methodology">​</a></h2>
<p>We developed a compact suite of well-scoped evaluations to establish current performance baselines. While the tasks are relatively simple, they already meaningfully stratify model performance. Unlike benchmarks that focus primarily on text generation (e.g., question answering, code generation), our evaluations emphasize <strong>tool calling capabilities</strong> — a core component of what makes Goose a powerful agent.</p>
<p>Tool calling enables models to interact with <a href="https://github.com/modelcontextprotocol/servers" target="_blank" rel="noopener noreferrer">MCP extensions</a> and make API calls, expanding Goose&#x27;s functionality beyond the base models. In many cases, tasks required multiple chained tool calls to reach completion. For instance, modifying a file involves finding it in your filesystem, viewing its contents, and then updating it. Each step must be executed correctly to complete the task effectively.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation-suites">Evaluation Suites<a href="#evaluation-suites" class="hash-link" aria-label="Direct link to Evaluation Suites" title="Direct link to Evaluation Suites">​</a></h3>
<p>Our evaluations are defined in the <a href="https://github.com/block/goose/tree/main/crates/goose-bench/src/eval_suites" target="_blank" rel="noopener noreferrer">Goose repository</a> (PRs welcome to add additional evals!) and are grouped into two categories:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="core-suite">Core Suite<a href="#core-suite" class="hash-link" aria-label="Direct link to Core Suite" title="Direct link to Core Suite">​</a></h4>
<p>These evals focus on certain tasks fundamental to developer workflows:</p>
<ul>
<li><strong>Create a file</strong>: Generate and save a new file</li>
<li><strong>List files</strong>: Access and display directory contents</li>
<li><strong>Developer Search/Replace</strong>: Search through a large file and make several replacements</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="vibes-suite">Vibes Suite<a href="#vibes-suite" class="hash-link" aria-label="Direct link to Vibes Suite" title="Direct link to Vibes Suite">​</a></h4>
<p>Designed as a &quot;vibe check&quot;, these tasks quickly reveal how well models perform with Goose on a broad variety of tasks. Some, like the Flappy Bird and Goose Wiki tasks are straightforwardly visually inspectable, making it easy to eyeball outputs across models:</p>
<ul>
<li><strong>Blog summary</strong>: Fetch a blog post and summarize key points</li>
<li><strong>Flappy Bird</strong>: Implement the game in Python 2D</li>
<li><strong>Goose Wiki</strong>: Create a Wikipedia-style webpage about Goose</li>
<li><strong>Restaurant research</strong>: Search for the best Sichuanese restaurants in NYC&#x27;s East Village</li>
<li><strong>Squirrel census</strong>: Perform data analysis on a CSV file</li>
</ul>
<p>This initial set of evaluations represents a carefully curated selection of manually designed tasks, chosen to highlight key strengths and weaknesses of models when integrated with Goose. However, this is just the beginning! Our goal is to continuously expand the Goosebench evaluation suite with high-quality, targeted tasks that provide deeper insights into model performance with Goose.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation-methodology">Evaluation Methodology<a href="#evaluation-methodology" class="hash-link" aria-label="Direct link to Evaluation Methodology" title="Direct link to Evaluation Methodology">​</a></h3>
<p>Each model was tested on the above <strong>8 tasks, with 3 runs per task</strong>, (totaling <strong>24 runs per model</strong>):</p>
<ul>
<li>Each evaluation consisted of a single turn prompt to Goose. While this benchmark focuses on single turn execution, future evaluations may assess multi-turn interactions and iterative improvement</li>
<li>Goose was required to autonomously complete the task using tool execution loops without user intervention</li>
<li>If Goose halted execution and asked the user for more guidance (e.g., &quot;I am going to write the following contents to the file. Should I continue?&quot;), this was considered the end of task completion. In such cases, Goose may not have successfully completed the task as measured by our evaluation framework, even if it was on the right track.</li>
<li>To account for output variability, each evaluation was run three times per model, allowing multiple chances for success.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scoring-and-evaluation-criteria">Scoring and Evaluation Criteria<a href="#scoring-and-evaluation-criteria" class="hash-link" aria-label="Direct link to Scoring and Evaluation Criteria" title="Direct link to Scoring and Evaluation Criteria">​</a></h3>
<p>We calculate each model&#x27;s leaderboard score by averaging its performance across all evaluation tasks. For each task, we run the model three times and normalize each run&#x27;s score to a 0-1 scale. The model&#x27;s task score is the average of these three runs. The final leaderboard score is the average of all task scores for that model.</p>
<p>Each evaluation is scored on a mix of criteria tailored to the specific task:</p>
<ol>
<li>
<p><strong>Tool Call Execution</strong>: Did the model make the correct tool calls to complete the task?</p>
</li>
<li>
<p><strong>LLM as a Judge</strong> (where applicable): Some evaluations used GPT-4o to assess response quality on a 0-2 scale. In these cases, we generated 3 GPT-4o assessments, took the most common score among them, and ran a fourth assessment if needed to break a tie to get the final score.</p>
<ul>
<li>0 points: Incorrect or fundamentally flawed</li>
<li>1 point: Partially correct, but with issues</li>
<li>2 points: Fully correct and well executed</li>
</ul>
</li>
<li>
<p><strong>Task Specific Criteria</strong>: Different tasks required different checks, such as:</p>
<ul>
<li>Correct output formatting (e.g., markdown, output to file)</li>
<li>Expected answers (e.g., correct insights in data analysis)</li>
<li>Valid implementation (e.g., valid Python code)</li>
</ul>
</li>
</ol>
<p>Some evaluations, like code execution or file creation, have clear pass/fail criteria, similar to unit tests. Others, such as blog summarization or restaurant research, require qualitative judgment rather than strict correctness. To assess both objective and open-ended tasks, we combine task-specific criteria, tool call verification, and (where applicable) LLM as a judge scoring.</p>
<p>To assess both objective and open-ended tasks, we combine task-specific criteria, tool call verification, and (where applicable) LLM-as-a-judge scoring. This approach maintains rigor where correctness is well-defined while allowing for nuanced evaluation of subjective outputs.</p>
<p>Our goal is to provide a directional signal of model performance rather than absolute accuracy, balancing concrete and qualitative criteria.</p>
<p>Additionally, we tracked:</p>
<ol>
<li>
<p><strong>Token Efficiency</strong>: Measures total tokens used in successful runs, providing insight into model efficiency and inference speed.</p>
</li>
<li>
<p><strong>Duration</strong>: Time to execute the task. This is not reflected in the leaderboard as it is significantly affected by differences across model inference providers and hardware.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="manual-inspection-and-observations-of-results">Manual Inspection and Observations of Results<a href="#manual-inspection-and-observations-of-results" class="hash-link" aria-label="Direct link to Manual Inspection and Observations of Results" title="Direct link to Manual Inspection and Observations of Results">​</a></h3>
<p>We manually inspected a handful of results to assess quality. Given the scale (768 total runs across 32 models), full manual validation of every evaluation run was infeasible. Key takeaways from our inspections:</p>
<ul>
<li>
<p>LLM-as-a-judge was reliable at identifying fully incorrect answers (0 points), but distinguishing between 1 and 2 points was more subjective.</p>
</li>
<li>
<p>Some tasks (e.g., blog summarization, restaurant searches) lacked automated factual verification. The evaluation framework could confirm whether a tool was called (e.g., web search executed) and the LLM judge could assess the instruction following to some degree, but our system overall had no way of verifying if the responses were factually correct.</p>
</li>
<li>
<p>Tool execution failures were a key source of performance variation, highlighting the importance of agentic capabilities in real-world AI tasks. A model might generate the correct output in chat, but if it fails to subsequently execute the right tools—such as writing the output to the right file as instructed by the user—the task is incomplete. This underscores the need for models to reliably perform multi-step actions autonomously, not just generate accurate responses.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="technical-challenges-with-open-models">Technical Challenges with Open Models<a href="#technical-challenges-with-open-models" class="hash-link" aria-label="Direct link to Technical Challenges with Open Models" title="Direct link to Technical Challenges with Open Models">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="context-length-limitations">Context Length Limitations<a href="#context-length-limitations" class="hash-link" aria-label="Direct link to Context Length Limitations" title="Direct link to Context Length Limitations">​</a></h3>
<p>A key limitation we encountered early on in our experimentation was the default context length in Ollama&#x27;s OpenAI-compatible endpoint (2048 tokens), which proved insufficient for most interactive agent scenarios.</p>
<p>Our system prompt alone consumes about 1,000 tokens, leaving limited space for user queries, context, and tool responses. This restriction hampers the model&#x27;s ability to manage long-running or complex tasks without losing essential context. While quantization (e.g., many Ollama models default to 4-bit) can reduce memory usage, it can also degrade performance.</p>
<p>However, we did not extensively explore the impact of different quantization levels. Fortunately, during our work, Ollama introduced an override that allowed us to increase the context length, helping to mitigate this limitation in our experiments.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tool-calling-inconsistencies-across-models">Tool Calling Inconsistencies Across Models<a href="#tool-calling-inconsistencies-across-models" class="hash-link" aria-label="Direct link to Tool Calling Inconsistencies Across Models" title="Direct link to Tool Calling Inconsistencies Across Models">​</a></h3>
<p>Different models have varying expectations for tool calling formats. For instance, Ollama requires JSON, while others like Functionary use XML. This lack of standardization poses integration challenges for inference providers, who must adapt the tool calling mechanisms for each model.</p>
<p>We observed performance fluctuations based on the model host and input/output formatting, highlighting the need for standardized tool calling formats in model training.
For models without native tool calling capabilities, we developed a &quot;toolshim&quot;—an interpretive layer that translates a model&#x27;s output into the appropriate tool calls.</p>
<p>This approach enables models like DeepSeek and Gemma to perform basic tool actions, though performance remains limited. None of the models configured with the toolshim greater than a 41% success rate in our experiments. Future improvements may focus on fine-tuning these shims for better handling of agentic tasks, helping to reduce inconsistencies across models in tool call generation.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="toolshims-to-bridge-the-gap">“Toolshims” to bridge the gap?<a href="#toolshims-to-bridge-the-gap" class="hash-link" aria-label="Direct link to “Toolshims” to bridge the gap?" title="Direct link to “Toolshims” to bridge the gap?">​</a></h3>
<p>We developed a &quot;toolshim&quot; as an experimental feature to enable models lacking native tool calling support (e.g., DeepSeek, Gemma3, Phi4) to interact with external tools. The toolshim pairs these models with a smaller, local model (e.g., mistral-nemo, qwen2.5-coder 7b), which is tasked with translating the primary model’s natural language responses into the appropriate tool calls for Goose to invoke. The local model is guided by Ollama’s structured outputs feature to enforce proper formatting for tool call generations.</p>
<p>However, this solution has limited performance due to:</p>
<ul>
<li>
<p><strong>Instruction-following limitations:</strong> The smaller models used typically have less robust instruction-following ability especially for longer inputs, making them prone to inaccuracies when parsing the primary model&#x27;s output into the correct tool calls. We also found the shim models to be quite sensitive to prompting.</p>
</li>
<li>
<p><strong>Structured output interference:</strong> Ollama’s structured output feature influences the model’s token sampling process, where the output is impacted by the model’s fundamental ability to extract information and generate JSON appropriately.</p>
</li>
</ul>
<p>Despite these challenges, there could be potential in fine-tuning these toolshim models to specifically optimize them for tool call generation.
If you’d like to try out the toolshim, check out our <a href="https://block.github.io/goose/docs/guides/experimental-features" target="_blank" rel="noopener noreferrer">documentation</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="practical-advice-for-local-model-users">Practical Advice for Local Model Users<a href="#practical-advice-for-local-model-users" class="hash-link" aria-label="Direct link to Practical Advice for Local Model Users" title="Direct link to Practical Advice for Local Model Users">​</a></h2>
<p>For those running a local, open-source AI experience with Goose, here are some key recommendations based on our testing:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimize-context-length">Optimize Context Length<a href="#optimize-context-length" class="hash-link" aria-label="Direct link to Optimize Context Length" title="Direct link to Optimize Context Length">​</a></h3>
<p>Ensure your model has enough context length to avoid running out of space in the context window. For Ollama, you can adjust the context length via an environment variable:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">OLLAMA_CONTEXT_LENGTH=28672 ollama serve</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can also set the context length as a parameter in Ollama by updating the Modlfile with your desired context length and running <code>ollama create</code>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="be-aware-of-quantization-levels">Be Aware of Quantization Levels<a href="#be-aware-of-quantization-levels" class="hash-link" aria-label="Direct link to Be Aware of Quantization Levels" title="Direct link to Be Aware of Quantization Levels">​</a></h3>
<p>Different quantization levels (4-bit, 8-bit, and 16-bit) have distinct impacts on performance:</p>
<ul>
<li><strong>4-bit:</strong> Offers maximum compression with minimal memory requirements but may degrade quality.</li>
<li><strong>8-bit:</strong> A balanced option for most consumer hardware, providing good performance and reasonable quality.</li>
<li><strong>16-bit:</strong> Higher quality but requires significantly more memory, which may limit performance on lower-end hardware.</li>
</ul>
<p>Ollama defaults to 4-bit quantization in most cases, but for tasks requiring more complex reasoning or tool usage, testing with higher quantization levels (e.g., 8-bit) may improve performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompting-matters-for-smaller-models">Prompting Matters for Smaller Models<a href="#prompting-matters-for-smaller-models" class="hash-link" aria-label="Direct link to Prompting Matters for Smaller Models" title="Direct link to Prompting Matters for Smaller Models">​</a></h3>
<p>Smaller models are more sensitive to prompt variations and often require more explicit instructions due to their limited capacity to infer. To achieve optimal performance, tasks may need to be broken down further, reducing ambiguity and limiting the range of possible responses.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hardware-considerations">Hardware Considerations<a href="#hardware-considerations" class="hash-link" aria-label="Direct link to Hardware Considerations" title="Direct link to Hardware Considerations">​</a></h3>
<p>We ran these models with a variety of inference providers (local and hosted) and hardware configurations including Apple M1, NVIDIA RTX 4080, NVIDIA RTX 4090, and NVIDIA H100. Due to the mix of hardware, we did not include measurements of task duration in the benchmark given the expected variability in inference performance driven by the underlying hardware.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-backends">GPU Backends<a href="#gpu-backends" class="hash-link" aria-label="Direct link to GPU Backends" title="Direct link to GPU Backends">​</a></h4>
<p>Depending on your hardware, different GPU acceleration backends offer varying levels of performance:</p>
<ul>
<li>
<p><strong>CUDA (NVIDIA GPUs)</strong>: Currently offers the best performance and compatibility for running LLMs locally. Most open models and inference frameworks are optimized for CUDA first.</p>
</li>
<li>
<p><strong>Metal (Apple Silicon)</strong>: Provides good acceleration on Mac devices with M-series chips. While not as fast as high-end NVIDIA GPUs, recent optimization work has made Metal increasingly viable for running 7B-13B models.</p>
</li>
<li>
<p><strong>ROCm (AMD GPUs)</strong>: Support is improving but still lags behind CUDA. If you have a compatible AMD GPU, you may expect to see some performance limitations and compatibility issues with certain models and quantization methods.</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="cpugpu-memory-management">CPU/GPU Memory Management<a href="#cpugpu-memory-management" class="hash-link" aria-label="Direct link to CPU/GPU Memory Management" title="Direct link to CPU/GPU Memory Management">​</a></h4>
<p>Ollama helps distribute model layers across CPU and GPU memory, allowing you to run larger models than would fit entirely in your GPU VRAM. However, be aware of:</p>
<ul>
<li><strong>Data movement overhead</strong>: When a model doesn&#x27;t fit entirely in GPU memory, constant data movement between CPU and GPU can significantly impact performance</li>
<li><strong>GPU utilization</strong>: Models that fit entirely in GPU memory will perform dramatically better than those that require CPU offloading</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="considering-cloud-hosted-open-models">Considering Cloud-Hosted Open Models?<a href="#considering-cloud-hosted-open-models" class="hash-link" aria-label="Direct link to Considering Cloud-Hosted Open Models?" title="Direct link to Considering Cloud-Hosted Open Models?">​</a></h3>
<p>If using a cloud service like OpenRouter to try larger open-weight models (e.g., LLaMA 3 70B or Qwen), be aware that performance may vary depending on which hosted inference provider you&#x27;re using.</p>
<p>Different providers might:</p>
<ul>
<li>Quantize models on the backend without clear disclosure</li>
<li>Implement different integration patterns that affect model performance, especially with tool calling</li>
<li>Have different hardware configurations affecting speed and reliability</li>
</ul>
<p>We recommend experimenting with different hosted inference providers to see which works best for your specific use cases. OpenRouter for example lets you <a href="https://openrouter.ai/docs/features/provider-routing" target="_blank" rel="noopener noreferrer">specify the provider</a> you want to route your requests to.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="run-your-own-benchmarks">Run Your Own Benchmarks<a href="#run-your-own-benchmarks" class="hash-link" aria-label="Direct link to Run Your Own Benchmarks" title="Direct link to Run Your Own Benchmarks">​</a></h2>
<p>We encourage the community to conduct their own benchmarks with various hardware setups and configurations to help deepen our understanding of how Goose performs across different setups. We also welcome contributions of additional evals to GooseBench to broaden our coverage.</p>
<p>We are currently cleaning up our code and  working on some quality of life improvements to make the process of running evals and reproducing these results more streamlined, and will share those when ready (next few weeks)!</p>
<p>Special thanks to our contributors, Zaki and Marcelle, for their work on GooseBench, which enabled this experimentation.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-work">Future Work<a href="#future-work" class="hash-link" aria-label="Direct link to Future Work" title="Direct link to Future Work">​</a></h2>
<p>As AI capabilities continue to evolve, we aim to systematically expand our evaluation framework to capture a broader range of use cases. We hope to benchmark models on a wider swath of consumer-grade hardware to better understand system requirements, execution times, and the impact of different quantization levels on performance.</p>
<p>We also plan to introduce vision-oriented evaluations, particularly for multimodal models with Goose. These will assess image processing, multimodal reasoning, and visual tool interactions, helping us measure how well models integrate and perform across different modalities.</p>
<p>In addition, we seek to develop evaluations tailored to non-developer workflows and tasks. This will provide insights into how Goose and AI models can serve a wider range of users beyond technical audiences.</p>
<p>Finally, we see value in testing long-context retention and multi-turn interactions to evaluate model performance in complex, sustained conversations.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="result-eval-examples">Result Eval Examples<a href="#result-eval-examples" class="hash-link" aria-label="Direct link to Result Eval Examples" title="Direct link to Result Eval Examples">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="flappy-bird">Flappy Bird<a href="#flappy-bird" class="hash-link" aria-label="Direct link to Flappy Bird" title="Direct link to Flappy Bird">​</a></h3>
<p>For runs that successfully created a working flappy bird game with pygame, here are the gifs of playing the games:</p>
<div class="carousel-container"><h3 class="carousel-header">claude-3-5-haiku</h3><div class="swiper swiper-container-flappy" style="width:40%"><div class="swiper-wrapper"><div class="swiper-slide"><img src="/goose/assets/images/claude-3-5-haiku-ff0b0347d5aec59efab98769039f7063.gif" alt="Slide 1" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/claude-3-5-sonnet-2-b445a205684b5c7bd674d744b92cc4ea.gif" alt="Slide 2" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/claude-3-7-sonnet-a11804cddf505cec1df78ca8315c8257.gif" alt="Slide 3" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/deepseek-chat-v3-0324-01d4dec7f79311b593ac04fbba9f93ef.gif" alt="Slide 4" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/deepseek-r1-toolshim-mistral-nemo-bf4bd9d06ff57563ae2762d047ccb1c7.gif" alt="Slide 5" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/gpt-4-5-preview-1cd4983ce3ceafaf22b12dff95ed4ad1.gif" alt="Slide 6" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/gpt-4o-mini-9e4478259ad6e3e4353394c0b3f19126.gif" alt="Slide 7" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/gpt-4o-cbdbf0c788605e797fb8f4ea9b987b8c.gif" alt="Slide 8" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/o1-134e3913d813a9e797e979c42a92fa3f.gif" alt="Slide 9" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/o3-mini-7a869a80bf99f27dd7454830ae309184.gif" alt="Slide 10" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/qwen2.5-coder-32b-ceca4ac06aaf0f42f3d6b65363c60e3e.gif" alt="Slide 11" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/qwq-3a05f474ec77049ba53a0439b36df0ba.gif" alt="Slide 12" class="carousel-image"></div></div><div class="swiper-button-prev"></div><div class="swiper-button-next"></div><div class="swiper-pagination"></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="wiki-pages">Wiki Pages<a href="#wiki-pages" class="hash-link" aria-label="Direct link to Wiki Pages" title="Direct link to Wiki Pages">​</a></h3>
<p>For runs that successfully created an index.html for the Wiki page task, here’s what the rendered outputs look like: Wiki pages Missing results are for models that did not successfully write to an index.html file. For example, they may have outputted the code to write in chat and asked the user to implement that code in an index.html file rather than written to the file themselves.</p>
<div class="carousel-container"><h3 class="carousel-header">gemma3.27b-toolshim-mistral-nemo</h3><div class="swiper swiper-container-wiki" style="width:80%"><div class="swiper-wrapper"><div class="swiper-slide"><img src="/goose/assets/images/gemma3.27b-toolshim-mistral-nemo-ee63d6bc421659fa3501a447f8c46426.png" alt="Slide 1" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/claude-3.5-haiku-71f1ce5faa13846a556ba539c57caae8.png" alt="Slide 2" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/claude-3.5-sonnet-2-2c683339b1e4880de5ce5aa435304a5a.png" alt="Slide 3" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/claude-3.7-sonnet-6042d99d5829957f3fd471d44ff2ea26.png" alt="Slide 4" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/deepseek-chat-v3-0324-879b3ea78b30ff135607365fd77eeebe.png" alt="Slide 5" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/deepseek-r1-distill-llama-70b-toolshim-mistral-nemo-bfd7f9d3f37a92aaae6c7afb74b52d91.png" alt="Slide 6" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/gpt-4.5-preview-99cafaf409ffbc47a8186ed0a5c05043.png" alt="Slide 7" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/gpt-4o-mini-37fbaf84ea5c2d7d57a9b4e7ddf4fb3b.png" alt="Slide 8" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/gpt-4o-8b238b9d92ca62e7480ab6f12faeedfa.png" alt="Slide 9" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/llama3.3.70b-instruct-q4_K_M-91e7e0436dd5b7530fc18ff39366369d.png" alt="Slide 10" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/llama3.3.70b-instruct-q8_0-9477c2fcd3abc509fa4855d9633be812.png" alt="Slide 11" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/mistral-nemo_index-4866f9a20a1294585adef2dc4fbc4ade.png" alt="Slide 12" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/o1-9a41312ec15fa512b13f215c94dc1335.png" alt="Slide 13" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/o3-mini-0da4fd28bcb174906ff6b2664c08e88d.png" alt="Slide 14" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/phi4-toolshim-mistral-nemo-db0e5f6075b793b2b6facfa483fc9a5d.png" alt="Slide 15" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/phi4-toolshim-qwen2.5-coder7b-632ebfc62d7e69db76bc030235990574.png" alt="Slide 16" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/qwen2.5-coder.14b-dfb90e9a247737df599d7736e982fa64.png" alt="Slide 17" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/qwen2.5-coder.32b-e540b5187a35326045a1810070188b68.png" alt="Slide 18" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/qwen2.5.14b-5f613200d90cb8f8e5242cefab130ed2.png" alt="Slide 19" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/qwen2.5.32b-cf589a3b4494e4f75a63f3b7c8f13923.png" alt="Slide 20" class="carousel-image"></div><div class="swiper-slide"><img src="/goose/assets/images/qwq-2df978b1c23d170ed2bf2d2caf5c3ecd.png" alt="Slide 21" class="carousel-image"></div></div><div class="swiper-button-prev"></div><div class="swiper-button-next"></div><div class="swiper-pagination"></div></div></div>
</div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/goose/blog/2025/04/01/mcp-nondevs"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">MCP Explained for Non-Developers</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/goose/blog/2025/03/31/securing-mcp"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Securing the Model Context Protocol</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#leaderboard" class="table-of-contents__link toc-highlight">Leaderboard</a></li><li><a href="#open-source-model-details" class="table-of-contents__link toc-highlight">Open Source Model Details</a></li><li><a href="#key-results" class="table-of-contents__link toc-highlight">Key Results</a></li><li><a href="#approach-and-methodology" class="table-of-contents__link toc-highlight">Approach and Methodology</a><ul><li><a href="#evaluation-suites" class="table-of-contents__link toc-highlight">Evaluation Suites</a></li><li><a href="#evaluation-methodology" class="table-of-contents__link toc-highlight">Evaluation Methodology</a></li><li><a href="#scoring-and-evaluation-criteria" class="table-of-contents__link toc-highlight">Scoring and Evaluation Criteria</a></li><li><a href="#manual-inspection-and-observations-of-results" class="table-of-contents__link toc-highlight">Manual Inspection and Observations of Results</a></li></ul></li><li><a href="#technical-challenges-with-open-models" class="table-of-contents__link toc-highlight">Technical Challenges with Open Models</a><ul><li><a href="#context-length-limitations" class="table-of-contents__link toc-highlight">Context Length Limitations</a></li><li><a href="#tool-calling-inconsistencies-across-models" class="table-of-contents__link toc-highlight">Tool Calling Inconsistencies Across Models</a></li><li><a href="#toolshims-to-bridge-the-gap" class="table-of-contents__link toc-highlight">“Toolshims” to bridge the gap?</a></li></ul></li><li><a href="#practical-advice-for-local-model-users" class="table-of-contents__link toc-highlight">Practical Advice for Local Model Users</a><ul><li><a href="#optimize-context-length" class="table-of-contents__link toc-highlight">Optimize Context Length</a></li><li><a href="#be-aware-of-quantization-levels" class="table-of-contents__link toc-highlight">Be Aware of Quantization Levels</a></li><li><a href="#prompting-matters-for-smaller-models" class="table-of-contents__link toc-highlight">Prompting Matters for Smaller Models</a></li><li><a href="#hardware-considerations" class="table-of-contents__link toc-highlight">Hardware Considerations</a></li><li><a href="#considering-cloud-hosted-open-models" class="table-of-contents__link toc-highlight">Considering Cloud-Hosted Open Models?</a></li></ul></li><li><a href="#run-your-own-benchmarks" class="table-of-contents__link toc-highlight">Run Your Own Benchmarks</a></li><li><a href="#future-work" class="table-of-contents__link toc-highlight">Future Work</a></li><li><a href="#result-eval-examples" class="table-of-contents__link toc-highlight">Result Eval Examples</a><ul><li><a href="#flappy-bird" class="table-of-contents__link toc-highlight">Flappy Bird</a></li><li><a href="#wiki-pages" class="table-of-contents__link toc-highlight">Wiki Pages</a></li></ul></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Quick Links</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/goose/docs/getting-started/installation">Install Goose</a></li><li class="footer__item"><a class="footer__link-item" href="/goose/extensions">Extensions</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/block-opensource" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/@blockopensource" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/company/block-opensource" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/blockopensource" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter / X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://bsky.app/profile/opensource.block.xyz" target="_blank" rel="noopener noreferrer" class="footer__link-item">BlueSky<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://njump.me/opensource@block.xyz" target="_blank" rel="noopener noreferrer" class="footer__link-item">Nostr<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/goose/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/block/goose" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Block, Inc.</div></div></div></footer></div>
<script src="/goose/inkeepChatButton.js" type="module"></script>
<div id="inkeepSearchBar"></div>
<script>(()=>{const e=document.getElementById("inkeepSearchBar");new MutationObserver((t=>{const n=document.getElementById("inkeep");n&&n.appendChild(e)})).observe(document.documentElement,{attributes:!0})})()</script>
<script src="/goose/inkeepSearchBar.js" type="module"></script></body>
</html>